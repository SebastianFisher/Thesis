{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79fa5939-b5d7-4212-83b4-894fd257f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS NOTEBOOK IS FOR LOADING THE PRETRAINED EM 300micron-by-300micron model FROM MATLAB\n",
    "\n",
    "# ALWAYS RUN THIS CELL\n",
    "# from AntennaNetwork import AntennaCNN#imports for next cells \n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "from EM300CNN import EM300CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c965a20-e961-43fe-a646-6e9149ab37a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO IMPORT MODEL PARAMS FROM MATLAB (not necessary if already run)\n",
    "model = EM300CNN()\n",
    "\n",
    "# Load matlab data as file\n",
    "f = h5py.File('em300_params.mat','r')\n",
    "\n",
    "# C is a matlab variable cell array storing the weights and biases of conv and forward layers\n",
    "weights_and_biases = f.get('C')\n",
    "\n",
    "# load batchnorm params from matlab file\n",
    "bn_params = f.get(\"BN_Params\") \n",
    "\n",
    "CONV_LAYERS = 12  # count of conv layers in model\n",
    "FC_LAYERS = 5    # count of fc layers\n",
    "\n",
    "# print(weights_and_biases)\n",
    "\n",
    "# This loop sets up the convolutional layer parameters of the model, weights and biases\n",
    "i = 0\n",
    "for name, params in list(model.named_parameters()):\n",
    "    if \"conv\" not in name:\n",
    "        continue\n",
    "\n",
    "    # Set convolutional layer params\n",
    "    # for every two entries, first is weights, second is biases\n",
    "    if i % 2 == 0:\n",
    "        layers_weights = torch.tensor(np.array(f[weights_and_biases[i,0]]))\n",
    "        layers_weights = torch.transpose(layers_weights, 3, 2) # transposing seems correct\n",
    "        # print(layers_weights.shape)\n",
    "    else:\n",
    "        layers_weights = torch.squeeze(torch.tensor(np.array(f[weights_and_biases[i,0]])))\n",
    "        # print(layers_weights.shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        params.data = nn.parameter.Parameter(layers_weights)\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "# This loop sets up the fully connected layer parameters of the model, weights and biases\n",
    "i = CONV_LAYERS*2\n",
    "for name, params in list(model.named_parameters()):\n",
    "    if \"fc_\" not in name:\n",
    "        continue\n",
    "\n",
    "    # Set forward layer params, again, every two entries, first is weights\n",
    "    # transpose because torch Linear stores weights this way \n",
    "    if i % 2 == 0:\n",
    "        layers_weights = torch.tensor(np.array(f[weights_and_biases[i,0]])).T\n",
    "        # print(layers_weights.shape)\n",
    "    else:\n",
    "        layers_weights = torch.squeeze(torch.tensor(np.array(f[weights_and_biases[i,0]])))\n",
    "        # print(layers_weights.shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        params.data = nn.parameter.Parameter(layers_weights)\n",
    "\n",
    "    i+=1\n",
    "\n",
    "# torch.save(model.state_dict(), \"./saved/AntennaCNN\")\n",
    "\n",
    "# This loop sets up the batch normalization layer parameters\n",
    "i = 0\n",
    "for name, m in model.named_children():\n",
    "    if 'batchnorm' not in name:\n",
    "        continue\n",
    "    # mean = self.running_mean\n",
    "    # variance = self.running_var\n",
    "    # gamma = self.weight\n",
    "    # beta = self.bias\n",
    "    # batchnorm_params[i] will be four separate arrays, in the order:\n",
    "    # running_mean, running_var, weight/gamme, beta/bias\n",
    "    # print(name)\n",
    "    running_mean = torch.tensor(np.array(f[f[bn_params[i,0]][0,0]])).squeeze()\n",
    "    running_var = torch.tensor(np.array(f[f[bn_params[i,0]][0,1]])).squeeze()\n",
    "    weight = torch.tensor(np.array(f[f[bn_params[i,0]][0,2]])).squeeze()\n",
    "    bias = torch.tensor(np.array(f[f[bn_params[i,0]][0,3]])).squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        m.running_mean = nn.parameter.Parameter(running_mean, requires_grad=False)\n",
    "        m.running_var = nn.parameter.Parameter(running_var, requires_grad=False)\n",
    "        m.weight = nn.parameter.Parameter(weight)\n",
    "        m.bias = nn.parameter.Parameter(bias)\n",
    "\n",
    "    i+=1\n",
    "\n",
    "torch.save(model.state_dict(), \"./saved/EM300CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2925ae92-343c-458f-bf84-50301ae511ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS TO LOAD THE MODEL AND TEST IT\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "model = EM300CNN()\n",
    "model.load_state_dict(torch.load(\"./saved/EM300CNN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc8e0b-d7fe-433b-971f-75cdb8375e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
