# -*- coding: utf-8 -*-
"""ReverseNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CQ45CjNFhupNrr9gHGlpmrA8DddlfUrl
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils, models
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from InverseNeuralNet import ReverseNN

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('./kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

class MNIST_data(Dataset):
    """MNIST data set"""

    def __init__(self, file_path,
                 transform = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(),
                     transforms.Normalize(mean=(0.5,), std=(0.5,))])
                ):

        df = pd.read_csv(file_path)

        if len(df.columns) == 784:
            # test data
            self.X = df.values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]
            self.y = None
        else:
            # training data
            self.X = df.iloc[:,1:].values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]
            self.y = torch.from_numpy(df.iloc[:,0].values)

        self.transform = transform

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        if self.y is not None:
            return self.transform(self.X[idx]), self.y[idx]
        else:
            return self.transform(self.X[idx])

train_data_path = "./data/train.csv"
test_data_path = ".//data/test.csv"

batch_size = 64

train_dataset = MNIST_data(train_data_path, transform= transforms.Compose(
                            [transforms.ToPILImage(), transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))]))
test_dataset = MNIST_data(test_data_path)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                           batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                           batch_size=batch_size, shuffle=False)

model = models.resnet50(pretrained=True)

def change_layers(model):
    model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    model.fc = nn.Linear(2048, 10, bias=True)
    return model

# change to take in new input

model = change_layers(model)

if(torch.cuda.is_available()):
    model = model.cuda()

# Load Forward network from saved state dict
MODELS_PATH = "./saved/"

model.load_state_dict(torch.load(MODELS_PATH + "MNISTConvNet"))


test_d = test_dataset[1].unsqueeze(0)
plt.imshow(test_dataset[1].squeeze(), cmap="gray")

if(torch.cuda.is_available()):
    test_d = test_d.cuda()

model.eval()
model(test_d)

import time
# Make Predictions on Test Data
"""
print('Predicting....')
start = time.time()

predictions = torch.LongTensor()
for i, data in enumerate(test_loader, 1):
    if (torch.cuda.is_available()):
        data = data.cuda()

    if(i%100 == 0):
        print('Batch {} done'.format(i))
    outputs = model(data)

    pred = outputs.cpu().data.max(1, keepdim=True)[1]
    predictions = torch.cat((predictions, pred), dim=0)

print('Completed in {} secs'.format(time.time() - start))
"""
device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)
print(f"Using {device} device")
input("enter to continue")

reverse_model = ReverseNN().to(device)

# X = torch.rand(1, 10, device=device)
# logits = reverse_model(X)
# pred_probab = nn.Softmax(dim=1)(logits)
# y_pred = pred_probab.argmax(1)
# print(f"Outputted Structure: {y_pred}")

# Create mock dataset of desired labels, one-hot vectors of length 10
# to indicate digit between 0 and 9
train_size = (100000,)
n = 10
indices = torch.randint(0, n, size=train_size)
generated_labels = torch.nn.functional.one_hot(indices, n).type(torch.float32)
# print(generated_labels[5])

batch_size = 64
reverse_train_loader = torch.utils.data.DataLoader(dataset=generated_labels,
                                           batch_size=batch_size, shuffle=True)

import time

print('Training....')
total = 0
correct = 0
start = time.time()

# Freeze Forward model params
for param in model.parameters():
  param.requires_grad = False

loss_fn = nn.CrossEntropyLoss()

optimizer = optim.Adam(reverse_model.parameters(), lr=0.001)

for epoch in range(1):

    for i, labels in enumerate(reverse_train_loader, 1):
        if(torch.cuda.is_available()):
            labels = labels.cuda()

        optimizer.zero_grad()

        # get batch outputs of inverse network
        outputs = reverse_model(labels)

        # get just class indices from labels (for cross entropy loss)
        classes = torch.argmax(labels, dim=1)

        # reshape into input for predictor neural network
        outputs = outputs.reshape([outputs.shape[0], 1, 28, 28])

        # Get digit value from forward neural network
        ground_truth_labels = model(outputs)

        generated_class = torch.argmax(ground_truth_labels, dim=1)
        total += len(classes) 
        correct += torch.sum(torch.eq(generated_class, classes))

        # Compute loss
        loss = loss_fn(ground_truth_labels, labels)

        if(i%100 == 0):
            print('Epoch: {} Batch: {} loss: {}'.format(epoch, i, loss.item()))
            print("Cumulative Correct/Total: {}/{}".format(correct, total))

        # Backpropagate gradients
        loss.backward()
        optimizer.step()

print('Training Completed in: {} secs'.format(time.time()-start))
print('Training accuracy: {} %'.format((correct/total)*100))

# Save current parameters of reverse net in drive
torch.save(reverse_model.state_dict(), MODELS_PATH + "MNISTInverseNet")


